{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is used to refine codebook learned by VQGAN\n",
    "By reducing size of codebook, reduce ambiguity in comparing and predicting operations happened in token embedding space\n",
    "Modify and validate in a dynamic way, k-means to determine representative centers, then iteratively construct code groupings\n",
    "Codebook quality is reflected by reconstruction performance\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from taming.vqgan import VQModel\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "from utils.bbox_utils import compute_iou_mask\n",
    "from utils.io import dump_json_object\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_path = '/mnt/data/jiangyong/vito/vqgan_embed.pth'\n",
    "code2mask_dir = '/mnt/data/jiangyong/vito/code2mask'\n",
    "code_embed = torch.load(codebook_path, map_location='cpu').numpy()\n",
    "code_smtic = torch.zeros((0, 256, 256), dtype=float)\n",
    "print('start parsing semantic masks of codebook')\n",
    "for code_name in tqdm(os.listdir(code2mask_dir)):\n",
    "    code = Image.open(os.path.join(code2mask_dir, code_name))\n",
    "    code = F.to_tensor(code)\n",
    "    code_smtic = torch.cat([code_smtic, code], dim=0)\n",
    "code_smtic = code_smtic.numpy()\n",
    "print(f'code semantic map is shaped at {code_smtic.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize groupings with k-means\n",
    "init_type = 'semantics'\n",
    "assert init_type in ['geometry', 'semantics']\n",
    "init_num = 100\n",
    "print(f'running K-means to initialize centers regarding {init_type} type')\n",
    "\n",
    "if init_type == 'geometry':\n",
    "    kmeans = KMeans(n_clusters=init_num).fit(code_embed)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    print(f'got initial centers at shape {centers.shape}')\n",
    "    centers_idx = []\n",
    "    # replace centers with representative codes\n",
    "    for i in range(init_num):\n",
    "        c = centers[i]\n",
    "        dist = np.linalg.norm(code_embed-c, axis=1)\n",
    "        centers_idx.append(np.argmin(dist))\n",
    "    centers = code_embed[centers_idx]\n",
    "else:\n",
    "    code_smtic_flatten = code_smtic.reshape(1024, -1)\n",
    "    kmeans = KMeans(n_clusters=init_num).fit(code_smtic_flatten)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    print(f'got initial centers at shape {centers.shape}')\n",
    "    centers_idx = []\n",
    "    # replace centers with representative codes\n",
    "    for i in range(init_num):\n",
    "        c = centers[i]\n",
    "        dist = np.linalg.norm(code_smtic_flatten-c, axis=1)\n",
    "        centers_idx.append(np.argmin(dist))\n",
    "    centers = code_embed[centers_idx]\n",
    "\n",
    "print(f'updated centers at shape {centers.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code grouping\n",
    "gamma = 0.05 * 256\n",
    "centers_smtic = code_smtic[centers_idx]\n",
    "grp = []   # denotes grouping label of each code\n",
    "novel_grp = init_num\n",
    "print('running code grouping')\n",
    "for i in tqdm(range(1024)):\n",
    "    c = code_embed[i]\n",
    "    s = code_smtic[i]\n",
    "    dist_smtic = np.linalg.norm(s-centers_smtic, axis=(1,2))\n",
    "    m = np.min(dist_smtic)\n",
    "    if m < gamma:\n",
    "        grp.append(np.argmin(dist_smtic))\n",
    "    else:\n",
    "        grp.append(novel_grp)\n",
    "        novel_grp += 1\n",
    "        centers = np.concatenate([centers, c[None,:]], axis=0)\n",
    "        centers_idx.append(i)\n",
    "        centers_smtic = np.concatenate([centers_smtic, s[None,:]], axis=0)\n",
    "print(f'got {novel_grp} semantic clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize codebook with color denoting groupings\n",
    "codebook_reduced = TSNE(n_components=2, learning_rate='auto').fit_transform(code_embed)\n",
    "color = np.zeros((novel_grp, 3))\n",
    "\n",
    "ch_list = [(novel_grp-1)//3] * 3\n",
    "if (novel_grp-1) % 3 == 1:\n",
    "    ch_list[0] += 1\n",
    "elif (novel_grp-1) % 3 == 2:\n",
    "    ch_list[0] += 1\n",
    "    ch_list[1] += 1\n",
    "\n",
    "color[1:ch_list[0]+1, 0] = np.linspace(0, 1, ch_list[0]+1)[1:]\n",
    "color[ch_list[0]+1:ch_list[0]+ch_list[1]+1, 1] = np.linspace(0, 1, ch_list[1]+1)[1:]\n",
    "color[-ch_list[2]:, 2] = np.linspace(0, 1, ch_list[2]+1)[1:]\n",
    "\n",
    "plt.scatter(codebook_reduced[:, 0], codebook_reduced[:, 1], s=1, c=color[grp])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('t-SNE on grouping clusters')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge codebook and validate quality\n",
    "ddconfig = {\n",
    "    'double_z': False,\n",
    "    'z_channels': 256,\n",
    "    'resolution': 256,\n",
    "    'in_channels': 3,\n",
    "    'out_ch': 3,\n",
    "    'ch': 128,\n",
    "    'ch_mult': [1,1,2,2,4],\n",
    "    'num_res_blocks': 2,\n",
    "    'attn_resolutions': [16],\n",
    "    'dropout': 0.0\n",
    "}\n",
    "vqgan_ckpt = '/mnt/data/jiangyong/vito/vqgan.ckpt'\n",
    "vqgan = VQModel(ddconfig=ddconfig, n_embed=novel_grp, embed_dim=256, ckpt_path=vqgan_ckpt)\n",
    "vqgan.to('cuda:1')\n",
    "vqgan.eval()\n",
    "vqgan.quantize.embedding.weight.data = torch.from_numpy(centers).to(vqgan.device)\n",
    "# test mask reconstruction\n",
    "mask_path = 'data/masks/refcoco/50.png'\n",
    "mask = F.to_tensor(Image.open(mask_path))\n",
    "mask = F.resize(mask, 256)\n",
    "\n",
    "H, W = mask.shape[-2:]\n",
    "mask_0 = F.crop(mask, 0, 0, 256, 256)\n",
    "mask_1 = F.crop(mask, (H-256)//2, (W-256)//2, 256, 256)\n",
    "mask_2 = F.crop(mask, H-256, W-256, 256, 256)\n",
    "crop_flag = np.argmax([torch.sum(mask_0), torch.sum(mask_1), torch.sum(mask_2)])\n",
    "mask = [mask_0, mask_1, mask_2][crop_flag]\n",
    "\n",
    "mask_vqgan = 2*mask - 1\n",
    "mask_vqgan = mask_vqgan.repeat(3, 1, 1).unsqueeze(0).to(torch.float).to(vqgan.device)\n",
    "with torch.no_grad():\n",
    "    encoding_indices = vqgan.encode(mask_vqgan)[-1][-1]\n",
    "print(f'encode input mask to sequence at length {len(encoding_indices)}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_mask = vqgan.decode_code(torch.LongTensor(encoding_indices.cpu()).to(vqgan.device),\n",
    "                                    shape=(1, 16, 16, -1))\n",
    "# vqgan reconstruction shape at [1, 3, 256, 256], value in [-1, 1]\n",
    "pred_mask = pred_mask.squeeze().detach().cpu().numpy()\n",
    "pred_mask = (pred_mask+1) / 2\n",
    "# ITU-R 601-2 luma transform: L = R * 0.299 + G * 0.587 + B * 0.114\n",
    "pred_mask = np.sum([[[0.299]], [[0.587]], [[0.114]]]*pred_mask, axis=0)\n",
    "print(f'decode sequence to mask at shape {pred_mask.shape}')\n",
    "\n",
    "vis_mask = np.concatenate([mask.squeeze().cpu().numpy(), pred_mask], axis=1)\n",
    "plt.imshow(vis_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running quantative evaluation to verify codebook quality\n",
    "mask_root = 'data/masks'\n",
    "datasets = ['refclef', 'refcoco', 'refcoco+', 'refcocog']\n",
    "for dataset in datasets:\n",
    "    pred_h5py_path = '/mnt/data/jiangyong/vito/pred_tmp.h5py'\n",
    "    pred_h5py = h5py.File(pred_h5py_path, 'w')\n",
    "    dataset_dir = os.path.join(mask_root, dataset)\n",
    "    print(f'running inference on {dataset} dataset')\n",
    "    for i, fname in enumerate(tqdm(os.listdir(dataset_dir))):\n",
    "        mask_path = os.path.join(dataset_dir, fname)\n",
    "        mask = F.to_tensor(Image.open(mask_path))\n",
    "        mask = F.resize(mask, 256)\n",
    "\n",
    "        H, W = mask.shape[-2:]\n",
    "        mask_0 = F.crop(mask, 0, 0, 256, 256)\n",
    "        mask_1 = F.crop(mask, (H-256)//2, (W-256)//2, 256, 256)\n",
    "        mask_2 = F.crop(mask, H-256, W-256, 256, 256)\n",
    "        crop_flag = np.argmax([torch.sum(mask_0), torch.sum(mask_1), torch.sum(mask_2)])\n",
    "        mask = [mask_0, mask_1, mask_2][crop_flag]\n",
    "\n",
    "        mask_vqgan = 2*mask - 1\n",
    "        mask_vqgan = mask_vqgan.repeat(3, 1, 1).unsqueeze(0).to(torch.float).to(vqgan.device)\n",
    "        with torch.no_grad():\n",
    "            encoding_indices = vqgan.encode(mask_vqgan)[-1][-1]\n",
    "            pred_mask = vqgan.decode_code(torch.LongTensor(encoding_indices.cpu()).to(vqgan.device),\n",
    "                                            shape=(1, 16, 16, -1))\n",
    "        # vqgan reconstruction shape at [1, 3, 256, 256], value in [-1, 1]\n",
    "        pred_mask = pred_mask.squeeze().detach().cpu().numpy()\n",
    "        pred_mask = (pred_mask+1) / 2\n",
    "        # ITU-R 601-2 luma transform: L = R * 0.299 + G * 0.587 + B * 0.114\n",
    "        pred_mask = np.sum([[[0.299]], [[0.587]], [[0.114]]]*pred_mask, axis=0)\n",
    "\n",
    "        gt_mask = mask.squeeze().cpu().numpy()\n",
    "\n",
    "        data_grp = pred_h5py.create_group(fname)\n",
    "        data_grp.create_dataset('pred', dtype='f', data=pred_mask)\n",
    "        data_grp.create_dataset('gt', dtype='f', data=gt_mask)\n",
    "\n",
    "        if i >= 100:\n",
    "            break\n",
    "    \n",
    "    pred_h5py.close()\n",
    "\n",
    "    pred_h5py = h5py.File(pred_h5py_path, 'r')\n",
    "    mask_iou = 0   # mIoU\n",
    "    mask_mat = np.zeros((0, 3))   # @0.5, @0.7, @0.9\n",
    "    print(f'evaluating metrics on {dataset} dataset')\n",
    "    for fname in tqdm(pred_h5py):\n",
    "        data_grp = pred_h5py[fname]\n",
    "        pred_mask = np.array(data_grp['pred'])\n",
    "        pred_mask = pred_mask >= 0.5\n",
    "        gt_mask = np.array(data_grp['gt']).astype(bool)\n",
    "        \n",
    "        iou = compute_iou_mask(pred_mask, gt_mask)\n",
    "        iou_thre = np.array([0.5, 0.7, 0.9])\n",
    "        acc = iou >= iou_thre\n",
    "\n",
    "        mask_mat = np.concatenate([mask_mat, acc.reshape(-1, 3)], axis=0)\n",
    "        mask_iou += iou\n",
    "    \n",
    "    total = mask_mat.shape[0]\n",
    "    mask_mIoU = mask_iou / total\n",
    "    mask_AP = np.sum(mask_mat, axis=0) / total\n",
    "    print(f'dataset: {dataset} | mask mIoU: {mask_mIoU} | mask AP: {mask_AP}\\n')\n",
    "\n",
    "    pred_h5py.close()\n",
    "    os.remove(pred_h5py_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save current results\n",
    "update_codebook = {\n",
    "    'size': novel_grp,\n",
    "    'embed': centers,\n",
    "    'center_id': centers_idx,\n",
    "    'grp_label': grp\n",
    "}\n",
    "torch.save(update_codebook, '/mnt/data/jiangyong/vito/refined_codebook.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize renewed codebook\n",
    "codebook_reduced = TSNE(n_components=2, learning_rate='auto').fit_transform(centers)\n",
    "color = np.zeros((novel_grp, 3))\n",
    "\n",
    "ch_list = [(novel_grp-1)//3] * 3\n",
    "if (novel_grp-1) % 3 == 1:\n",
    "    ch_list[0] += 1\n",
    "elif (novel_grp-1) % 3 == 2:\n",
    "    ch_list[0] += 1\n",
    "    ch_list[1] += 1\n",
    "\n",
    "color[1:ch_list[0]+1, 0] = np.linspace(0, 1, ch_list[0]+1)[1:]\n",
    "color[ch_list[0]+1:ch_list[0]+ch_list[1]+1, 1] = np.linspace(0, 1, ch_list[1]+1)[1:]\n",
    "color[-ch_list[2]:, 2] = np.linspace(0, 1, ch_list[2]+1)[1:]\n",
    "\n",
    "plt.scatter(codebook_reduced[:, 0], codebook_reduced[:, 1], s=1, c=color)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('t-SNE on renewed codebook')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate semantic distance matrix\n",
    "smtic_dist_mat = np.zeros((novel_grp, novel_grp))\n",
    "for i in tqdm(range(novel_grp)):\n",
    "    smtic_dist_mat[i] = np.linalg.norm(centers_smtic[i]-centers_smtic, axis=(1,2))\n",
    "smtic_dist_mat"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30457de8cc6bae60d89ad8bbc8792e699a89df9550d72b84de89fbbfa853c482"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('vito')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
